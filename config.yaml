# Databricks MLOps Configuration
# SECURITY NOTE: Credentials are read from environment variables for security
# Set these in GitHub Secrets or local .env file:
#   DATABRICKS_HOST - Databricks workspace URL (required for Databricks mode)
#   DATABRICKS_TOKEN - Databricks access token (required for Databricks mode)
#   DATABRICKS_WORKSPACE_PATH - Workspace path (optional, defaults below)
#   DATABRICKS_EXPERIMENT_PATH - MLflow experiment path (optional, defaults below)
#   Testing Deploy to Prod Testing 
#
# The code automatically reads from environment variables and overrides these values
# These values are only used as defaults if environment variables are not set
databricks:
  host: "https://dbc-5e289a33-a706.cloud.databricks.com"  # Default - overridden by DATABRICKS_HOST env var
  token: ""  # Must be set via DATABRICKS_TOKEN environment variable (NEVER hardcode!)
  workspace_path: "/Users/nbatink@gmail.com/gnu-mlops/liveprod"  # Default - overridden by DATABRICKS_WORKSPACE_PATH env var

# MLflow Configuration
mlflow:
  gnu_mlflow_config: "/Users/nbatink@gmail.com/gnu-mlops/experiments"  # Default - overridden by DATABRICKS_EXPERIMENT_PATH env var
  model_name: "workspace.default.gnu-mlops-model"
  tracking_uri: "databricks"  # Can be overridden via MLFLOW_TRACKING_URI env var

# Model Configuration
model:
  algorithm: "random_forest"  # Options: random_forest, xgboost, lightgbm, neural_network
  hyperparameters:
    n_estimators: 100
    max_depth: 10
    random_state: 42
  
# Training Configuration
training:
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  cv_folds: 5

# Data Configuration
data:
  source: "databricks"  # Options: databricks, s3, local
  table_name: "default.training_data"  # Update with your table name
  features:
    - feature1
    - feature2
    - feature3
  target: "target_column"
  
# Deployment Configuration
deployment:
  model_stage: "GNU_Production"  # Options: Staging, GNU_Production
  endpoint_name: "gnu-mlops-endpoint"
  compute_config:
    min_workers: 1
    max_workers: 4
    instance_type: "i3.xlarge"

# Monitoring Configuration
monitoring:
  enable_drift_detection: true
  log_predictions: true
  alert_email: "nbetini@gmail.com"

# Retraining Configuration
retraining:
  enabled: true  # Enable automatic retraining
  interval_days: 30  # Retrain every 30 days
  auto_deploy: true  # Automatically deploy if new model is better
  min_improvement: 0.0  # Minimum accuracy improvement to deploy (0.0 = any improvement)
  notification_email: "nbetini@gmail.com"  # Email for retraining notifications

# Batch Inference Configuration
batch_inference:
  enabled: true  # Enable daily batch inference
  input_path: "data/batch_input/"  # Path to input data files (file or directory)
  output_path: "data/batch_output/"  # Path to save prediction results
  archive_path: "data/batch_archive/"  # Path to archive processed input files
  model_stage: "GNU_Production"  # Model stage to use (Staging or GNU_Production)
  batch_size: 10000  # Process large files in batches of this size
  archive_input: true  # Archive input files after processing
  notification_email: "nbetini@gmail.com"  # Email for batch inference notifications

